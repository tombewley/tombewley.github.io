---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

First-year PhD student in the department of Engineering Mathematics at the University Bristol, supervised by [Jonathan Lawry](https://research-information.bristol.ac.uk/en/persons/jonathan-lawry(3aac8b8f-816c-4203-ba4b-b091bf4ddef4).html) and [Arthur Richards](https://research-information.bristol.ac.uk/en/persons/arthur-g-richards%28d4aa20a8-75fa-4b1a-8400-b2387ed04fe5%29.html) and supported by [Thales](https://www.thalesgroup.com/en). My project is tentatively entitled *Explainable AI for Interacting Autonomous Agents*. I am a graduate of the UK's top-ranked general engineering [degree](http://www.bristol.ac.uk/engineering/interdisciplinary/engineering-design/), holder of an [MSc](http://www.bristol.ac.uk/study/postgraduate/2018/eng/msc-adv-computing-machine-learning/) in Machine Learning, and recipient of an RAEng Engineering Leaders [Scholarship](https://www.raeng.org.uk/grants-and-prizes/schemes-for-students/engineering-leaders-scholarship). 

My PhD research explores how we might begin to understand and explain the policies of black-box autonomous agents, whose internal mechanisms and representations may be very different from our own, with a particular view to revealing the biases and flaws in their decision-making. Critical to this question are the putative trade-off between comprehensibility and performance of machine learning models, and the thorny relationship between correlation and causation in observed data whose generative origins are unknown.

I'm currently looking at using decision trees to 'clone' black-box policies and offer factual, counterfactual and narrative explanations of their behaviour. Check out [this pre-print](https://www.researchgate.net/publication/340051129_Modelling_Agent_Policies_with_Interpretable_Imitation_Learning) for some early thoughts and results. 