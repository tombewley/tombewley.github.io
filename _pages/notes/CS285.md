---
title: "CS285"
permalink: /notes/CS285
author_profile: true
---

#Content/Course on "deep [Reinforcement Learning](Reinforcement%20Learning), decision making and control" by [Sergey Levine](../../../Roam/Sergey%20Levine.md) and others at [UC Berkeley](../../../Roam/UC%20Berkeley.md).

[http://rail.eecs.berkeley.edu/deeprlcourse/](http://rail.eecs.berkeley.edu/deeprlcourse/)

## Lecture 1: Introduction and Course Overview
Deep learning helps us *handle unstructured environments* but is traditionally used merely for pattern recognition; reinforcement learning provides a *general formalism for behaviour* in terms of observations, actions and rewards, that aligns well with animal behaviour and experimental neuroscience. Deep RL combines the two, providing an end-to-end method for learning behaviour and decision making. There is a strong analogy to how computer vision has switched from hand-crafted processing stages to end-to-end deep learning. Deep RL is our current best bet for the 'master' algorithm that can solve the problem of intelligent behaviour in general.

The end-to-end learning of representations, informed by the final task application, is the key feature of the deep learning paradigm. This distinguishes it from traditional approaches in which the representation hierarchy is constructed by hand. Deep RL brings end-to-end representation learning into the control context.
- Most problems we think of as AI problems can be cast as RL, although this is not always the most efficient thing to do.

For most interesting real-world control problems, external rewards are extremely sparse and exploration is hard. We need more than just the reward-maximising policy update logic of RL, and [Inverse Reinforcement Learning](Inverse%20Reinforcement%20Learning), [Imitation Learning](Imitation%20Learning), [Transfer Learning](Transfer%20Learning) and [Meta Learning](Meta%20Learning) are all important pieces of the puzzle. Results from [Cognitive Science](Cognitive%20Science) also point to the importance of prediction for control. This motivates work on [Model-based Reinforcement Learning](Model-based%20Reinforcement%20Learning).

Open problems: deep RL is currently slow, transfer learning to reuse past knowledge is far from solved, it's not always clear what the reward function should be.

## Lecture 2: Supervised Learning of Behaviours

### OLD

Given an observation $o_t$ of an underlying state $s_t$ at time $t$, we are interested in producing an action $a_t$ via a policy function $\pi_\theta(a_t\vert o_t)$, which is parameterised by $\theta$. The trickiness comes from the fact that our choice of action will influence what $o_{t+1}$ will be. States satisfy the Markov property, but since observations may be incomplete or corrupted, they do not in general.

Given a dataset of $o$s and $a$s produced by some other system (e.g. a human), we can try to learn a mapping using standard supervised learning techniques. This is called *behavioural cloning*. The method is quite brittle, since any noise in observations lead us to veer ever further away from any single training trajectory and produce unpredictable behaviour. This stability problem is an example of *distributional shift*. In theory, this could be solved by the following procedure called DAgger:

1. Train $\pi_\theta(a_t\vert o_t)$ from target system (human) data $\mathcal{D}=\{o_1,a_1,...,o_N,a_N\}$.
2. Run $\pi_\theta(a_t\vert o_t)$ to get dataset $\mathcal{D}_\pi=\{o_1,...,o_M\}$ (only observations needed).
3. Ask human to label $\mathcal{D}_\pi$ with actions.
4. Aggregate $\mathcal{D}\leftarrow\mathcal{D}\cup\mathcal{D}_\pi$ and repeat.

Though in practice step 3 is clearly going to be very labour intensive if we are indeed using humans as the target, and also doesn't work if the initial learned policy is so bad that it produces trajectories that we can't even label properly.

Can we make behavioural cloning work without more data? Theoretically no, but practically yes through hacks such as sampling a wider range of trajectories from the target system, including those that are intentionally sub-optimal. 

Why can't we use behavioural cloning to solve everything? Well, humans can only provide a limited amount of data, and we're not good at everything. To go beyond this, we need to introduce the idea of costs and rewards.

### NEW

In this course we adopt the POMDP formalisation with Markovian states $s_t$, possibly non-Markovian observations $o_t$, actions $a_t$, and policies $\pi_\theta(a_t\vert o_t)$ which are observation-conditioned distributions over actions parameterised by $\theta$. Here is a useful causal DAG representation:

![[Screenshot from 2021-04-19 20-13-01.png]]

How can we learn policies? The most obvious solution to those familar with supervised learning is [Behavioural Cloning](Behavioural%20Cloning): collect $(o,a)$ pairs from an expert demonstrator and train a vanilla supervised learning model. This is the most basic form of [Imitation Learning](Imitation%20Learning).

BC suffers from a stability problem called [Covariate Shift](Covariate%20Shift) ($p_{data}(o_t)\neq p_{\pi_\theta}(o_t)$). It is theoretically proven that [DAgger](A%20Reduction%20of%20Imitation%20Learning%20and%20Structured%20Prediction%20to%20No-Regret%20Online%20Learning) progressively mitigates this problem over time. 

We can also improve imitation learning by simply trying to fit the expert data better. 
- If the expert policy is non-Markovian, we may benefit from using a longer observation history, although this can actually exacerbate the problem of [Causal](Causal) confusion (see [Causal Confusion in Imitation Learning](Causal%20Confusion%20in%20Imitation%20Learning).
- If the expert policy is multi-modal: we can think carefully about how to parameterise our imitation policy, for example using a mixture distribution or autoregressive discretisation.

But ultimately, the fundamental approach of imitation learning is limited by the quality of the expert demonstrator and the availability of data. If humans can learn autonomously from their own experience, can machines do the same?

## Lecture 3: TensorFlow and Neural Nets Review Session
*Skipped*

## Lecture 4: Introduction to Reinforcement Learning
An MDP augments the definition of a Markov chain (state space $\mathcal{S}$, transition operator $\mathcal{T}:\mathcal{S}\times\mathcal{S}\rightarrow[0,1]$) with an action space $\mathcal{A}$ and reward function $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$. A POMDP further  augments this with an observation space $\mathcal{O}$ and emission probabilities $p(o_t\in\mathcal{O}\vert s_t\in\mathcal{S})$. We use $\theta$ to denote the parameters of an MDP policy $\pi$. Given a policy, the joint distribution over state-action sequences $\tau$ of length $T$ can be decomposed via the Markov property: 

$$p_\theta(\tau)=p_\theta(s_1,a_1,...,s_T,a_T)=p(s_1)\prod_{t=1}^T\pi_\theta(a_t\vert s_t)p(s_{t+1}\vert s_t,a_t).$$

The objective of RL is to find a policy that maximises the expectation of the sum of reward (return) over trajectories:

$$\theta^*=\underset{\theta}{\text{argmax}}\ \mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^Tr(s_t,a_t)=\underset{\theta}{\text{argmax}}\ J(\theta).$$

An important fact is that even if the reward function is discontinuous, this expectation can still be smooth and differentiable in $\theta$, because the policy itself is smooth and differentiable (for neural networks at least).

The generic cycle followed is (1) Generate samples by running a policy, (2) Fit a predictive model to those samples (i.e. dynamics or value model) (3) Improve the policy based on the model predictions. These three steps may be cheap or expensive depending on exactly how they are implemented.

The Bellman equation is a crucial decomposition of the return, used by many RL algorithms: 

$$J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^Tr(s_t,a_t)=\mathbb{E}_{s_1\sim p_\theta(s_1)}[\mathbb{E}_{a_1\sim\pi(a_1\vert s_1)}[r(s_1,a_1)+\mathbb{E}_{s_2\sim p(s_2\vert s_1,a_1)}[...\vert s_1,a_1]\vert s_1]].$$

What if we *knew* the value inside the second expectation? Let us call this $Q^\pi(s_1,a_1)$. $$\mathbb{E}_{s_1\sim p_\theta(s_1)}[\mathbb{E}_{a_1\sim\pi(a_1\vert s_1)}[Q^\pi(s_1,a_1)\vert s_1]].$$ In this case, the decision about how to improve $\pi_\theta(a_1\vert s_1)$ (step 3 of the generic cycle) would be easy: simply move in the direction of increasing $Q$! Many RL algorithms involve the estimation of $Q$ functions from data, so that such updates can be performed approximately.

A whirlwind tour of RL algorithms:
- Policy gradient: *directly* differentiate the RL objective to learn a policy
- Value-based: estimate a $Q$ (or $V$) function and represent policies *implicitly* (e.g. as an $\text{argmax}$).
- Actor-critic: a *hybrid* in which a learned $Q$/$V$ function is used as part of the policy update step.
- Model-based: estimate the transition model and use it either for direct planning, policy improvement via backpropagation, generation of synthetic experience for a model-free learner, or something else (many variants).

None of these is universally best, and many tradeoffs exist: 
- Sample efficiency (off-policy algorithms are generally more efficient because they can reuse old data) and wall clock time (not the same thing!) 
- Stability and ease of use (understanding the convergence properties of many popular RL algorithms remains an open problem).
- Assumptions (stochastic vs deterministic environments, continuous vs discrete actions, finite vs infinite horizon, full vs partial observability).
- Particularities of the environment affecting the easy of policy, value and model representation.

## Lecture 5: Policy Gradients
In order to use gradient-based methods to maximise the RL objective $J(\theta)$, we need to know its derivative. For notational convenience, let us rewrite $J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^T r(s_t,a_t)=\int p_\theta(\tau)r(\tau)d\tau$. We can now write the derivative as

$$\nabla_\theta J(\theta)=\int \nabla_\theta p_\theta(\tau)r(\tau)d\tau.$$

Next, we make use of a convenient identity: $p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau)=p_{\theta}(\tau) \frac{\nabla_{\theta} p_{\theta}(\tau)}{p_{\theta}(\tau)}=\nabla_\theta p_\theta(\tau)$. Applying this identity in reverse,

$$\nabla_\theta J(\theta)=\int p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau)r(\tau)d\tau=\mathbb{E}_{\tau\sim p_\theta(\tau)}\nabla_{\theta}\log p_{\theta}(\tau)r(\tau).$$

Returning to the definition of $p_{\theta}(\tau)$ and taking the logarithm:

$$\log p_\theta(\tau)=\log p(s_1)+\sum_{t=1}^T\log\pi_\theta(a_t\vert s_t)+\log p(s_{t+1}\vert s_t,a_t).$$

Now taking the derivative w.r.t. $\theta$, all of the $p$ terms disappear:$$\nabla_\theta\log p_\theta(\tau)=\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t).$$We arrive at the following equation for the policy gradient:

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\left(\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t)\right)\left(\sum_{t=1}^Tr(s_{t},a_{t})\right).$$

All of these terms we can either query directly, or estimate from experience. A useful intuition for the policy gradient objective is as a *weighted maximum likelihood*; we want to make good trajectories (high $r(\tau)$) more likely, and bad trajectories less likely.
- If we follow this derivation for partially-observable environments, we end up with exactly the same result! Vanilla policy gradient can therefore be used in POMDPs without modification.

The `REINFORCE` algorithm performs a basic gradient descent update on the policy gradient: $\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta)$. It is not used very often in practice, because of the high variance of empirical estimates of the gradient.

One "free lunch" way of reducing variance is to introduce the simple causal fact that behaviour at time $t$ cannot affect reward at time $t'<t$, so any apparent relationship will cancel out in expectation. 

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\sum_{t'=t}^Tr(s_{t'},a_{t'})\right)$$

By removing terms from the sum we reduce variance, because there are fewer places to go wrong! The quantity in the reduced inner sum is sometimes called the *reward-to-go*, and is given the symbol $\hat{Q}$.

Another form of variance comes from the lack of invariance to reward translation: if all rewards are increased or decreased by a constant, the empirical policy gradient can change, even though such translation has no effect on the optimal policy or the direction of the gradient in expectation. A common way of resolving the translation problem is to use a *baseline*, which subtracts a carefully-chosen constant $b$ from $r(\tau)$ so as to reduce the empirical variance. Now consider the definition of the variance:

$$\text{Var}[J(\theta)]=\mathbb{E}_{\tau\sim p_\theta(\tau)}[(\nabla_{\theta}\log p_{\theta}(\tau)(r(\tau)-b))^2]-\mathbb{E}_{\tau\sim p_\theta(\tau)}[\nabla_{\theta}\log p_{\theta}(\tau)(r(\tau)-b))]^2.$$

We can ignore the second term, which will be the same for any $b$, since we know that baselines are unbiased in expectation. By taking the derivative and setting it to zero, we find that the optimal variance-minimising baseline is


$$b=\frac{\mathbb{E}_{\tau\sim p_\theta(\tau)}[(\nabla_\theta\log p_\theta(\tau))^2r(\tau)]}{\mathbb{E}_{\tau\sim p_\theta(\tau)}(\nabla_\theta\log p_\theta(\tau))^2}$$

The optimal baseline is actually a vector with a different component for every parameter in $\theta$, so is a little unweildy to use. 
- In practice, it is far more common to estimate the state value function $V^\pi(s)$ and use this as a baseline. In this case, the baseline-translated reward-to-go is called the *advantage* $A^\pi(s,a)$.

The other major problem with vanilla policy gradient is that it is strictly *on-policy*. We must throw away all of our old samples whenever the policy is changed! This can be very inefficient. If we want to use off-policy samples (i.e. from parameters $\theta'\neq \theta$), we can use a technique called *importance sampling*. This adds a reweighting term to the objective:

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\left(\prod_{t=1}^{T} \frac{\pi_{\theta}\left(a_{t}\vert s_{t}\right)}{\pi_{\theta'}\left(a_{t}\vert s_{t}\right)}\right)\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(a_{t}\vert s_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(s_{t}, a_{t}\right)\right)\right]$$

The variance of the importance sampling term increases exponentially with $T$. There are several practical first-order approximations that don't give the correct policy gradient, but might still be reasonable if $\theta\approx\theta'$ (more on these later).

## Lecture 6: Actor-Critic Algorithms
Actor-critic algorithms augment the policy gradient approach with value functions.

In vanilla policy gradient algorithms, the reward-to-go $\hat{Q}(s_t,a_t)$ serves as an estimate of the expected sum of future reward if we taken action $a$ in state $s$. These algorithms use a single sample in place of this expectation, which can have very high variance if the policy and dynamics have significant stochasticity. We can reduce variance, at the cost of some bias, by learning a function to approximate the expectation $Q^\pi$.
- The key idea of function approximation is generalisation: information "leaks out" to affect predictions for nearby states, hopefully yielding better estimates than if no leakage

Recall the common use of the state value function $V^\pi(s_t)$ as a baseline, which yields the advantage function $A_\pi(s_t,a_t)$. Is it better to learn $V^\pi$, $A^\pi$ or $Q^\pi$ for this implementation? Consider their relationship:

$$A^\pi(s_t,a_t)=Q^\pi(s_t,a_t)-V^\pi(s_t)=\mathbb{E}_{s_{t+1},r_t}[r(s_t,a_t)+V^\pi(s_{t+1})]-V^\pi(s_t)$$
For a particular sample, we can make an approximation and use the *actual* reward and next state in place of the expectation:

$$A^\pi(s_t,a_t)\approx r(s_t,a_t)+V^\pi(s_{t+1})-V^\pi(s_t)$$

This is written entirely in terms of $V^\pi$, so let's just learn this! (Alternatives considered later.)

The most basic way of learning $V^\pi$ is using training data of the form $\{(s_t,\sum_{t'=t}^Tr(s_{t'},a_{t'})\}$ (i.e. Monte Carlo samples). By defining the loss as (for example) the mean squared error, we have a standard regression problem. 

There's still a lot of variance in this approach, specifically in the training data itself. We can eliminate even more variance using a technique known as *bootstrapping*, in which $V^\pi$ is trained iteratively, using its own predictions for $s_{t+1}$ to construct labels. Training data at iteration $k$ now have the form $\{(s_t,r(s_t,a_t)+V^\pi_{\phi_{k-1}}(s_{t+1})\}$, where $\phi_{k-1}$ are the parameters of $V^\pi$ at iteration $k-1$. Although the labels will initially be very biased, the hope is that the process will converge to good estimates.
- In infinite-horizon problems, it becomes important to introduce a discount factor $\gamma$ at this point to prevent bootstrapped value estimates from continually increasing. Training data then have the form $\{(s_t,r(s_t,a_t)+\gamma V^\pi_{\phi_{k-1}}(s_{t+1})\}$.
	- One way of thinking about discounting is as modifying the MDP to introduce the concept of [Mortality](Mortality.md) via an absorbing zero-reward "death" state that is transitioned to from every state with probability $1-\gamma$.

For practical implementation of actor-critic algorithms, we face the problem that training data collected online will be highly-correlated. A common approach is to have multiple *workers* that deploy the current policy in parallel to collect partially-decorrelated data for central retraining. This may be done synchronously (update all workers at the same time) or asynchronously (update at different times, which can be made more efficient but slightly violates the on-policy assumption). The use of asychronous workers is the central feature of the [A3C](A3C) algorithm. 

Note that $V^\pi$ can be used in *two ways* to assist in policy learning (note $\gamma$ now included):
- As a baseline to subtract from the Monte Carlo return:
	- Note that this has *zero* bias, because it is a constant with respect to the parameters $\theta$.

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\left(\sum_{t'=t}^T\gamma^{t'-t}r(s_{t'},a_{t'})\right)-V^\pi(s_t)\right)$$

- To replace the baselined return with a [TD error](TD%20error) (lower variance than above but additional bias):

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(r(s_t,a_t)+\gamma V^\pi(s_{t+1})-V^\pi(s_t)\right)$$

We can generalise these two approaches to use $n$-step Monte Carlo returns before switching to $V^\pi$ , giving us the ability to control the tradeoff between variance (first term) and bias (second term):

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\left(\sum_{t'=t}^{t+n}\gamma^{t'-t}r(s_{t'},a_{t'})\right)+\gamma^nV^\pi(s_{t+n})-V^\pi(s_t)\right)$$

And we can generalise even further to take a weighted average over all possible $n$. If we take the weights $w_n\propto\gamma^{n-1}$ where $\gamma<1$ (i.e. exponential falloff), it is possible to write this weighted average as

$$\nabla_\theta J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_{t=1}^T\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\sum_{t'=t}^{T}(\gamma\lambda)^{t'-t}\left(r(s_{t'},a_{t'})+\gamma V^\pi(s_{t'+1})-V^\pi(s_{t'})\right)\right)$$

This approach is called *generalised advantage estimation* (GAE).

## Lecture 7: Value Function Methods
We start our journey here by noticing that by definition, the $\text{argmax}$ of the true advantage $A^\pi(s_t,a_t)$ is *at least as good* as sampling from the current policy $\pi$, regardless of what $\pi$ actually is. This means that as long as we have a good estimate of the advantage, we should be able to iteratively improve our policy by taking the $\text{argmax}$ in this way. As a result, we never actually need to represent a policy explicitly, and can omit direct learning via the policy gradient. This general approach is called *policy iteration*. 

What form of the value function should we learn to perform policy iteration? Consider the two options for learning from transition samples $(s,a,r,s')$:

$$V^\pi(s)\leftarrow r+\gamma\mathbb{E}_{s'\sim p(s'\vert s,\pi(s))}[V^\pi(s)]$$

$$Q^\pi(s,a)\leftarrow r+\gamma\mathbb{E}_{s'\sim p(s'\vert s,a)}[Q^\pi(s',\pi(s'))]$$

Notice how to learn the state value function $V^\pi$, we need to sample from the transition dynamics given the policy $p(s'\vert s,\pi(s))$. This requires a *model of the environment*. Crucially, however, for $Q^\pi$, we can just use the *actual* action $a$ and next state $s'$ from the collected sample, so no such model is required. So let's use $Q^\pi$!

If for the $\pi(s')$ sample we deterministically take the $\text{argmax}$ of the current $Q^\pi(s',a')$ estimate, policy iteration reduces to *value iteration*. In the deep RL context, it tends to be referred to as *fitted Q-iteration* (FQI). Letting $\phi$ be the parameters of our learned $Q$ function, the FQI update given a batch of $n$ samples $(s_i,a_i,s'_i)$ is

$$\phi\leftarrow \phi-\alpha\sum_{i=1}^n\frac{\partial}{\partial\phi}(Q_\phi(s_i,a_i)-[r(s_i,a_i)+\gamma\max_{a'}Q_\phi(s'_i,a')])$$

- Two parameters of FQI are how many value function updates to complete before collecting more data with the updated ($\text{argmax}$) policy, and how many samples $n$ to collect each time. If both of these parameters $=1$, we have a fully online algorithm.
- At a theoretical level FQI allows us to get away with using *off-policy* data, because the only part of the update that depends on $\pi$ is the sampling of $a'$, which is done at learning time. 
	- However, in practice we do still require the data to cover the state and action spaces relatively thoroughly (*broad support*), otherwise we may fail to discover better actions to take or have large errors in unvisited states. This motivates the introduction of randomness into the data-collecting policy, with simple approaches being $\varepsilon$-greedy and a softmax function over $Q$ values (Boltzmann exploration).
		- Much more on exploration later...

The lecture then goes on to discuss [Convergence of Value-based RL](Convergence%20of%20Value-based%20RL).

## Lecture 8: Deep RL with $Q$-functions
Practical deep RL algorithms that use value functions have to tackle two problems that hamper the stability of $Q$-learning:
- **Online samples are strongly correlated**: This violates the i.i.d. assumption of gradient-based optimisation, and gives the network time to *locally overfit* to the most recent data. We could parallelise data collection as in actor-critic, but more common is to use a *replay buffer* from which we sample batches i.i.d. Any data collection policy will work as long as it has broad support, but it's a good idea to gradually refresh the buffer with data from a policy close to the one being learned.
- **$Q$ learning is only semi-gradient descent**: We don't compute the gradient through the target value $[r(s_i,a_i)+\gamma\max_{a'}Q_{\phi}(s_i',a')]$, which effectively "changes out from under us". Depending on the number of gradient steps we take on each update, we either don't converge at all; or converge to the wrong targets. The solution is a compromise, achieved by maintaining a second *target $Q$ network* with parameters $\phi'$ that are infrequently updated to match $\phi$. By using the target network to create $\max_{a'}Q_{\phi'}(s',a')$ for regression, we achieve much more stable learning.
	- More recently it has been popular to update $\phi'$ in a lagging way using Polyak averaging in parameter space. While in general neural networks to be highly nonlinear in their parameters, there is some partial theoretical justification for this being a reasonable thing to do, as long as $\phi$ and $\phi'$ remain reasonably close.

These additions (plus the decision to update $\phi$ on every timestep with a single sampled batch) lead us to the classic deep $Q$ learning algorithm `DQN`. We can also present $Q$ online learning as a special case with a replay buffer of size $1$, and zero lag in the target parameters $\phi'$.
- In general, a whole family of $Q$ learning algorithms is defined by varying the rates of three processes: (1) buffer refreshing, (2) $Q$ function regression, (3) target parameter update.

An interesting thing to note is that while the ordering of learned $Q$ values across the action space tends to converge to the true ordering in successful learning, **the absolute values are often systematically way too high**! This problem, known as *overestimation in $Q$ learning*, is actually pretty easy to get our heads around:
- For any two random variables $X_1,X_2$, we have that $\mathbb{E}[\max(X_1,X_2)]\geq\max[\mathbb{E}[X_1],\mathbb{E}[X_2]]$ because the former systematically selects for positive deviations from the average. This generalises to any number of variables. 
- Therefore, $\mathbb{E}\max_{a'}Q_{\phi'}(s',a')=\mathbb{E}Q_{\phi'}(s',\text{argmax}_{a'}Q_{\phi'}(s',a'))\geq\max_{a'}\mathbb{E}Q_{\phi'}(s',a')$.

The solution is to *partially decorrelate* the selection of $a'$ from the evaluation of its value. We could do this by using two pairs of value and target networks with parameters $\phi_A,\phi'_A,\phi_B,\phi'_B$, but in practice the common approach is even simpler: just use $\phi$ to select $a'$! The update becomes

$$\phi\leftarrow \phi-\alpha\sum_{i=1}^n\frac{\partial}{\partial\phi}(Q_{\phi}(s_i,a_i)-[r(s_i,a_i)+\gamma\max_{a'}Q_{\phi'}(s'_i,\text{argmax}_{a'}Q_{\phi}(s',a'))])$$

This practically-trivial modification yields the `Double DQN` algorithm, which does a pretty good, if incomplete, job of mitigating the overestimation problem.

As in actor-critic, another trick that can be used to improve $Q$ learning algorithms is to use $n$-step returns in the target values. This allows us to control the bias-variance tradeoff. Using $n$-step returns is **only strictly valid in the on-policy setting** because multi-step returns are policy-dependent. Common responses are:
- Ignore the problem;
- Dynamically choose $n$ to only include returns where actions match the greedy ones;
- Importance sampling.

Can we extend $Q$-learning to continuous actions? The difficulty comes when trying to find the $\text{max}_{a'}$ over $Q(s',a')$. A very simple approach is to approximate this with the max over a finite set of randomly-sampled actions. Fancier approaches include:
- Cross-entropy sampling (CEM; more on this when discussing model-based methods).
- Constraining the functional form of the $Q$ function to make analytical maximisation possible (e.g. quadratic in $a$).
- Learning an approximate maximiser. Although commonly presented as an actor-critic algorithm, this is actually exactly what `DDPG` does!

$Q$-learning is often harder to stabilise than policy gradient methods. Some practical tips:
- Large replay buffers help.
- Expect a long time spent doing no better than random early in learning.
- Start with high exploration and gradually reduce.
- Bellman error gradients can be large; clipping gradients or using the Huber loss can help.
- Double Q-learning helps a lot in practice. It's super easy to implement and has almost no downsides.
- $n$-step returns can also help early in training, but does have a downside as it biases the objective.
- Scheduled learning rates and optimisers such as `Adam` can help.
- Run multiple random seeds; learning can be very inconsistent.

## Lecture 9: Advanced Policy Gradients
We can reinterpret policy gradient methods as doing something quite similar to policy iteration - alternating between advantage evaluation and policy update - but performing a much *gentler* update than jumping to the $\text{argmax}$. This might be desirable when we are uncertain about the accuracy of our advantage estimate.

This lecture we dig a little deeper: when and why do policy gradient methods work? First, let us restate the fundamental RL objective (now including the discount factor $\gamma$):

$$J(\theta)=\mathbb{E}_{\tau\sim p_\theta(\tau)}\sum_t\gamma^tr(s_t,a_t)=\mathbb{E}_{s_0\sim p(s_0)}V^{\pi_\theta}(s_0).$$

By definition, policy improvement means maximising $J(\theta')-J(\theta)$. We can show that this difference is exactly the expectation of the advantage function for $\theta$, under the trajectory distribution for $\theta'$:

$$J(\theta')-J(\theta)=\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}\sum_t\gamma^tA^{\pi_\theta}(s_t,a_t).$$

To do this requires a few tricks. First, we notice that we can freely change the expectation distribution, because initial state probabilities are policy-independent. Next, we add and subtract values for all other $t\geq 0$, which has no effect on the result because everything cancels out:

$$J(\theta')-\mathbb{E}_{s_0\sim p(s_0)}V^{\pi_\theta}(s_0)=J(\theta')-\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}V^{\pi_\theta}(s_0)=J(\theta')-\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^\infty\gamma^tV^{\pi_\theta}(s_t)-\sum_{t=1}^\infty\gamma^tV^{\pi_\theta}(s_t)\right].$$

Now, we refactor the terms in the two infinite sums:

$$J(\theta')-\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}V^{\pi_\theta}(s_0)=J(\theta')+\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^\infty\gamma^t(\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t}))\right].$$

Finally, we substitute in the definition of $J(\theta')$ and regroup again:

$$\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}\sum_{t=0}^\infty\gamma^tr(s_t,a_t)+\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^\infty\gamma^t(\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t}))\right]=\mathbb{E}_{\tau\sim p_{\theta'}(\tau)}\left[\sum_{t=0}^\infty\gamma^t(r(s_t,a_t)+\gamma V^{\pi_\theta}(s_{t+1})-V^{\pi_\theta}(s_{t}))\right].$$

The stuff inside the brackets is exactly the definition of the advantage function! 

But hold up... in policy gradient methods the expectation is taken under the trajectory distribution for the *old* parameters $\theta$. Practical algorithms rely on the fact that we can assume this distribution mismatch is small, because $p_\theta$ is close to $p_{\theta'}$ when $\pi_\theta$ is close to $p_{\theta'}$. 
- How can we show this is true? For the simple case when $\pi_\theta$ is deterministic (but $\pi_{\theta'}$ may not be), an identical analysis to that for [Covariate Shift](../../../Roam/Covariate%20Shift.md) in behavioural cloning allows us to show that $p_{\theta'}(s_t)-p_\theta(s_t)\leq2\epsilon t$, where $\epsilon$ is the maximum probability that $\pi_{\theta'}$ takes a different action to $\pi_\theta$ in any state. For the general case, let $\pi_{\theta'}$ be *close* to $\pi_\theta$ if $\vert\pi_{\theta'}(a_t\vert s_t)-\pi_{\theta}(a_t\vert s_t)\vert\leq\epsilon\ \forall s_t$, where $\vert\cdot\vert$ denotes the total variation divergence. In this case, a bit of careful though reveals that the probability of a "mistake" is still just $\epsilon$, so again, $p_{\theta'}(s_t)-p_\theta(s_t)\leq2\epsilon t$.

This theoretical analysis demonstrates the importance of ensuring that $\vert\pi_{\theta'}(a_t\vert s_t)-\pi_{\theta}(a_t\vert s_t)\vert\leq\epsilon$. If we can do that for small enough $\epsilon$, the policy gradient update is guaranteed to give us a positive $J(\theta')-J(\theta)$. The total variation divergence is a little unweildy to work with (partly because the absolute value operator is non-differentiable), but it turns out that it is upper bounded by the KL divergence $D_\text{KL}(\pi_{\theta'}\vert\vert\pi_\theta)$, so we can use that instead. 

There are several ways of enforcing a KL divergence constraint during policy learning, but one of the most popular is to take the second-order Taylor expansion $D_\text{KL}(\pi_{\theta'}\vert\vert\pi_\theta)\approx\frac{1}{2}(\theta'-\theta)\textbf{F}(\theta'-\theta)$, where $\textbf{F}$ is the Fisher information matrix. Since vanilla gradient descent already effectively regularises under the L2 norm, this approximate constraint can be enforced by a simple reweighting, yielding the *natural gradient*:

$$\theta'=\theta+\alpha\textbf{F}^{-1}\nabla_\theta J(\theta),$$

with $\alpha=\sqrt{\frac{2\epsilon}{\nabla_\theta J(\theta)^T\textbf{F}\nabla_\theta J(\theta)}}$. The effect of $\textbf{F}$ is to apply a linear transformation to the gradient vector according to the influence of each parameter on the policy. $\textbf{F}$ can be found approximately using samples, allowing this method to be usable in practical algorithms such as `TRPO`.

## Lecture 10: Model-based Planning
In this lecture we switch gears to talk about how *known* models of the environmental dynamics $p(s_{t+1}\vert s_t,a_t)$ can be used for optimal control and planning. Typically, solutions to this problem do not explicitly talk about policy functions, but just about action sequences $a_1,...,a_T$ over some time horizon $T$. For the case of deterministic transition dynamics $f$, the objective can be written as 

$$\underset{a_1,...,a_T}{\text{argmax}}\sum_{t=1}^T\Big(r(s_t,a_t)\ \text{s.t.}\ s_{t+1}=f(s_t,a_t)\Big),$$

and can be generalised to stochastic dynamics as follows:

$$\underset{a_1,...,a_T}{\text{argmax}}\ \mathbb{E}\left[\Big(\sum_{t=1}^Tr(s_t,a_t)\Big)\Big\vert a_1,...,a_T\right].$$

*Open-loop* planning algorithms attempt to tackle the objective all in one go, solving for a sequence of actions $a_1,...,a_T$ which is then executed in full:
- **Random shooting**: Sample many action sequences from some distribution (e.g. Gaussian) and evaluate the return for each. Actually works pretty well for low-dimensional systems and can easily be parallelised (good for deep learning systems).
- **Cross-entropy method (CEM)**: Start with random shooting, then iteratively update the sampling distribution toward those samples that give higher return. A common approach here is to refit the distribution to the top-performning $x\%$ of the samples, which are called the *elites*.
	- Check out CMA-ES as an extension of CEM that uses momentum.
- **Derivative-based methods**: Rather than sampling, attempt to obtain an analytical solution by differentiating known dynamics equations. 
	- This often faces stability challenges as derivatives with respect to earlier actions are likely to be much higher than later ones.
	- A special case that does not suffer from this issue is the **Linear-quadratic regulator (LQR)**, devised for linear deterministic transition functions and quadratic reward functions (common in robotics). The equations can be solved by some fairly verbose, but not too complicated, linear algebra. The general idea is to iterate backward from the planning horizon to obtain a full expression for the policy in matrix form, then iterate forward from the current timetep, plugging states into these matrices and recovering the optimal actions. 
		- Note that if LQR is being used in an MPC control scheme (below), we only need to run forward one timestep, though the speed gain from this is negligible.
		- The LQR can be extended to stochastic dynamics under certain assumptions. For example, if noise is Gaussian, it provably cancels out during the backward iteration step, which therefore remains unchanged. The only difference is that we keep the entire policy structure and look up based on the actual state encountered at each timestep.
		- It can also be extended for nonlinear dynamics, in which case it is known as *differential dynamic programming* (DDP) or iterative LQR (iLQR). This involves taking a second-order Taylor expansion around a given state and action, approximating the system as locally linear/quadratic, finding the optimal solution, and iteratively repeating until convergence.

However, in the stochastic case, choosing a whole sequence of actions in an open-loop manner is very bad idea, because it is likely that information will be revealed to us in future that reduces our uncertainty about future states. *Closed-loop* planning algorithms attempt only to return the single next action $a_1$, and are re-run on every timestep:
- **Model-predictive control (MPC)**: A generic name for a broad family of algorithms that use some open-loop planning method (e.g. LQR), but then discard all but the first action. 
- **Monte Carlo tree search (MCTS)**: At each depth, simulate a baseline policy from each leaf and evaluate return. Many algorithms exist for deciding whether or not to expand a node further on the basis of average return and number of evaluations (as a measure of uncertainty). Note that the stochastic nature of MCTS makes it very difficult to analyse theoretically, and the algorithm offers few theoretical guarantees, but it often works very well in practice. 

## Lecture 11: Model-based Reinforcement Learning
Now we consider the case where a dynamics model is not known, and we instead want to construct it from data using supervised learning. When attempting to deploy the most obvious naive approach to model learning, using a dataset collected from an exploration policy $\pi_0$ and using the result to construct a policy $\pi_f$, we immediately re-encounter the problem of distributional shift: $p_{\pi_f}(s_t)\neq p_{\pi_0}(s_t)$. `DAgger`-like approaches again tend to be successful in mitigating this problem.

We should always expect some error in a learned dynamics model, so while more expensive, closed-loop control methods such as MPC tend to be far more robust. In general, the more often we re-plan, the less perfect each individual plan needs to be, so even random sampling can work well.

Overfitting is a major problem with learned models, because any spurious peaks and troughs are likely to be mercilessly exploited by a planning algorthm. This motivates introducing some form of *uncertainty estimation*, which in turn allows us to plan actions that have high value in expectation or within some lower confidence bound. How can we build uncertainty-aware neural network models?
- Bad idea: Use output entropy (e.g. if model parameterisation is a softmax distribution over discrete states or a Gaussian over continuous ones). This is problematic because the entropy is itself subject to estimation error, and can also be exploited by a planner! The output entropy is the *wrong kind of uncertainty*: [aleatoric instead of epistemic](../../../Aleatoric%20and%20Epistemic%20Uncertainty.md), which is actually what we care about here.
- What we really want is to represent a distribution over model parameters, which is the realm of [Bayesian](../../../Roam/Bayesian.md) learning. Using a [Bayesian neural network](Bayesian%20neural%20network.md) is the principled way of doing this, but far simpler (and seemingly more effective for model-based RL!) is to train an ensemble of models on independently-sampled subsets of the training set and combining their outputs.
	- In fact, the combination of SGD and random network initialisation are often enough to yield sufficiently diverse models, so we can even train them on exactly the same data!
	- In formal terms, the ensemble method approximates the model parameter distribution as a mixture of Dirac deltas centred on the particular weights in each trained network.
		- Crucially, when combining the outputs of an ensemble of (e.g.) Gaussian models, we should not just take the mean prediction across all models, but treat the overall model as a mixture of Gaussians.

Given a model that is aware of epistemic uncertainty (whether Bayesian network or ensemble), the general approach to planning is to first sample a set of parameters $\theta$ (for ensembles this amounts to picking one at random), run the planning algorithm, repeat $N$ times, and accumulate the average return for each candidate action sequence.

How can we learn models in environments with image observations? This is a hard problem because images have high dimensionality, lots of redundancy, and are almost always only partial observations of the true environmental state. A common approach is to attempt to separately learn an emission model $p(o_t\vert s_t)$ and a latent state space model $p(s_{t+1}\vert s_t,a_t)$. Let $\phi$ denote the conjunction of parameters from both of these models. In standard (fully-observed) model learning, the objective is 

$$\underset{\phi}{\max}\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\log p_\phi(s_{i,t+1}\vert s_{i,t},a_{i,t}),$$

but in latent space model learning, we have an additional expectation:

$$\underset{\phi}{\max}\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\mathbb{E}_{s,s'\sim p(s,s'\vert o_{i,1:T},a_{i,1:T})}\Big[\log p_\phi(s'\vert s,a_{i,t})+\log p_\phi(o_{i,t}\vert s)\Big].$$

Many options exist for learning $p(s,s'\vert o_{i,1:T},a_{i,1:T})$, which is often referred to as the *encoding* distribution. Solutions range from conditioning only on the current observation $o_{i,t}$ (single-step encoder; simplest but least accurate) to conditioning on the entire state-action sequence (required when observations are heavily partial).

- Considering the doubly-simple case of a deterministic single-step encoder $s_{i,t}=g_\psi(o_{i,t})$, the objective equation reduces to a fully-differentiable form that can be trained with backpropagation.

	$$\underset{\phi,\psi}{\max}\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\log p_\phi(g_\psi(o_{i,t+1})\vert g_\psi(o_{i,t}),a_{i,t})+\log p_\phi(o_{i,t}\vert g_\psi(o_{i,t})).$$
	
An alternative to latent space models, which has become a little more popular recently is to learn the dynamics directly in image space. This requires more data and bigger models, but less principled forethought about their structure (i.e. the classic deep learning tradeoff).

## Lecture 12: Model-based Policy Learning
## Lecture 13: Exploration (Part 1)
## Lecture 14: Exploration (Part 2)
## Lecture 15: Offline Reinforcement Learning
## Lecture 16: Introduction to RL Theory
## Lecture 17: Deep RL Algorithm Design
## Lecture 18: Probability and Variational Inference Primer
## Lecture 19: Connection between Inference and Control
## Lecture 20: Inverse Reinforcement Learning
## Lecture 21: Transfer Learning and Multi-Task Learning
## Lecture 22: Meta-Learning
## Lecture 23: Challenges and Open Problems