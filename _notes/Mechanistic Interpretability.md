---
title: Mechanistic Interpretability
permalink: /Mechanistic Interpretability
collection: notes
---
Embodies a fundamental belief that neural networks are only black boxes by default, and that we can understand their function in terms of interpretable concepts and algorithms if we try hard enough.

[This post](https://www.alignmentforum.org/posts/64MizJXzyvrYpeKqm/sparsify-a-mechanistic-interpretability-research-agenda) draws an analogy to the three stages of software reverse engineering:
- Choose a **mathematical** description among (probably) many equivalent ones (e.g. neurons, [polytopes](Interpreting%20Neural%20Networks%20through%20the%20Polytope%20Lens), [directions](Linear%20Representation%20Hypothesis)).
- Assign approximate **semantic** labels to elements of that description. 
- **Validate** the semantic description by prediction and intervention.

The goal of mechanistic interpretability research is to push the [Pareto front](Pareto%20Efficiency) of the faithfulness (via validation) and the length of the semantic description.



